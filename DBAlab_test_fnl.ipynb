{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBAlab_test_fnl",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lV0wyEforJK3LLrlmCKXk3xhw13TskHx",
      "authorship_tag": "ABX9TyNPMtS9+oBRNS+mc5BVHAvi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS8hl0_Npv8F"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from torch.autograd import Variable\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZVTJIHvp1iY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Nah_Ifp1fk"
      },
      "source": [
        "#import all dataset\n",
        "\n",
        "df1 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject1.csv\", header=None)\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject2.csv\", header=None)\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject3.csv\", header=None)\n",
        "df4 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject4.csv\", header=None)\n",
        "df5 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject5.csv\", header=None)\n",
        "df6 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject6.csv\", header=None)\n",
        "df7 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject7.csv\", header=None)\n",
        "df8 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject8.csv\", header=None)\n",
        "df9 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject9.csv\", header=None)\n",
        "df10 = pd.read_csv(\"/content/drive/My Drive/DBALab-Test/Preprocessed/mHealth_subject10.csv\", header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlQ2ErKRsQCj"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RuJtdEFnYWi"
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pibqSSKtm0pO"
      },
      "source": [
        "#creating the virtual client\n",
        "import syft as sy\n",
        "import torch as th\n",
        "hook = sy.TorchHook(th)\n",
        "from torch import nn, optim\n",
        "\n",
        "n_clients = 8\n",
        "clients = []\n",
        "for i in range(n_clients):\n",
        "    client_name = 'client{}'.format(i)\n",
        "    client = sy.VirtualWorker(hook, id = client_name)\n",
        "    clients.append(client)\n",
        "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt4m0tcVm2wX"
      },
      "source": [
        "secure_worker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UwWasULov3m"
      },
      "source": [
        "#concate all data to one dataset\n",
        "\n",
        "df1['client_name'] = 1\n",
        "df2['client_name'] = 2\n",
        "df3['client_name'] = 3\n",
        "df4['client_name'] = 4\n",
        "df5['client_name'] = 5\n",
        "df6['client_name'] = 6\n",
        "df7['client_name'] = 7\n",
        "df8['client_name'] = 8\n",
        "df9['client_name'] = 9\n",
        "df10['client_name'] = 10\n",
        "df_all = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10])\n",
        "\n",
        "#split to data train and test\n",
        "df_train = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8])\n",
        "df_test = pd.concat([df9,df10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6dzxeWOqAfs"
      },
      "source": [
        "#drop the 0 label\n",
        "df_trainx = df_train[df_train[21]!=0]\n",
        "nolabel_tx = df_train[df_train[21]==0]\n",
        "df_testx = df_test[df_test[21]!=0]\n",
        "nolabel_tsx = df_test[df_test[21]==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JkHmtw3vxER"
      },
      "source": [
        "train_data = np.array(df_trainx, dtype = np.float32)\n",
        "test_data = np.array(df_testx, dtype = np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqQ-G784v-Li"
      },
      "source": [
        "def print_dataset(name, data):\n",
        "    print('Dataset {}. Shape: {}'.format(name, data.shape))\n",
        "    print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KElpHsOJv_8U"
      },
      "source": [
        "print_dataset('Train', train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TZ_w61rwSqA"
      },
      "source": [
        "def get_input_and_output(data):\n",
        "    input = Variable(torch.tensor(data[:, :-2], dtype = torch.float32))\n",
        "    output1 = Variable(torch.tensor(data[:, -2], dtype = torch.float32))\n",
        "    return input, output1\n",
        "\n",
        "input, output1= get_input_and_output(train_data)\n",
        "test_input, test_output1 = get_input_and_output(test_data)\n",
        "\n",
        "to_percent = lambda x: '{:.2f}%'.format(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BKg_lJY708I"
      },
      "source": [
        "print_dataset('Train', output1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKgh3q_S5kao"
      },
      "source": [
        "#create model in this case, use LogisticRegression\n",
        "input_size = 21\n",
        "learning_rate = 0.01\n",
        "num_iterations = 20000\n",
        "\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wISuilgDm2zw"
      },
      "source": [
        "#send the data to each client\n",
        "client_features = []\n",
        "client_targets = []\n",
        "for i in range(n_clients):\n",
        "    train_data2 = df_trainx[df_trainx['client_name']==i+1]\n",
        "    train_data2 = np.array(train_data2, dtype = np.float32)\n",
        "    train_data3 = th.tensor(train_data2, dtype = torch.float32, requires_grad=True)\n",
        "    features = train_data3[:, :-2].clone().detach().requires_grad_(True)\n",
        "    targets = train_data3[:, -2][:, None].clone().detach() \n",
        "    client_features.append(features.send(clients[i]))\n",
        "    client_targets.append(targets.send(clients[i]))\n",
        "model = LogisticRegression()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDqMUo94zNOK"
      },
      "source": [
        "Define some functions to train the machine learning model in a federated way while keeping track of the training loss and the training accuracy.\n",
        "The whole process is done in a trusted aggregator, in 100 iterations. (We can vary the number of iterations.). At each iteration, a copy of the shared model is sent to all the 8 client. Each client trains its own local model with its own local dataset, in 5 local iterations. (We can vary the number of local iterations.) Each local model improves a little bit in its own direction. Then we compute the local losses and local accuracies to keep track of them. So, we will able to create graphs of the learning curves: Training Losses versus Iterations and Training Accuracies versus Iterations. We send the local models to the trusted aggregator that will average all the model updates. This averaged model is the shared model that is sent to all the 8 client at the begining of each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQIzT6agm23V"
      },
      "source": [
        "iterations = 100 #2000 #can vary the number of iteration\n",
        "worker_iterations = 5 #can vary the number of local iteration\n",
        "\n",
        "#create function for grap\n",
        "def plot_federated_graphs(losses, accuracies):\n",
        "    for i in range(n_clients):\n",
        "        plt.plot(losses[i], label=f'Client {i}')\n",
        "    legend = plt.legend(loc='upper right', shadow=True)\n",
        "    plt.title(f\"Training Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.show()\n",
        "    for i in range(n_clients):\n",
        "        plt.plot(accuracies[i], label=f'Client {i}')\n",
        "    legend = plt.legend(loc='lower right', shadow=True)\n",
        "    plt.title(f\"Training Accuracy\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy (Percent %)\")\n",
        "    plt.show()\n",
        "\n",
        "#create function for accuracy    \n",
        "def compute_federated_accuracy(model, input, output):\n",
        "    prediction = model(input)\n",
        "    n_samples = prediction.shape[0]\n",
        "    s = 0.\n",
        "    for i in range(n_samples):\n",
        "        p = 1. if prediction[i] >= 0.5 else 0.\n",
        "        e = 1. if p == output[i] else 0.\n",
        "        s += e\n",
        "    return 100. * s / n_samples\n",
        "\n",
        "#create function for run federated learning\n",
        "def federated_learning(client_features, client_targets, test_input, test_output):\n",
        "    model = LogisticRegression()\n",
        "    criterion = torch.nn.BCELoss(size_average=True)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "    losses = [[] for i in range(n_clients)]\n",
        "    accuracies = [[] for i in range(n_clients)]\n",
        "    for iteration in range(iterations):\n",
        "        models = [model.copy().send(clients[i]) for i in range(n_clients)]\n",
        "        optimizers = [torch.optim.SGD(params = models[i].parameters(), lr = learning_rate) for i in range(n_clients)]\n",
        "        for worker_iteration in range(worker_iterations):\n",
        "            last_losses = []\n",
        "            for i in range(n_clients):\n",
        "                optimizers[i].zero_grad()\n",
        "                prediction = models[i](client_features[i])\n",
        "                loss = criterion(prediction, client_targets[i])\n",
        "                loss.backward()\n",
        "                optimizers[i].step()\n",
        "                loss = loss.get().data.item()\n",
        "                last_losses.append(loss)\n",
        "        for i in range(n_clients):\n",
        "            losses[i].append(last_losses[i])\n",
        "            train_acc = compute_federated_accuracy(models[i], client_features[i], client_targets[i])\n",
        "            accuracies[i].append(train_acc)\n",
        "            models[i].move(secure_worker)\n",
        "        with th.no_grad():\n",
        "            avg_weight = sum([models[i].linear.weight.data for i in range(n_clients)]) / n_clients\n",
        "            model.linear.weight.set_(avg_weight.get())\n",
        "            avg_bias = sum([models[i].linear.bias.data for i in range(n_clients)]) / n_clients\n",
        "            model.linear.bias.set_(avg_bias.get())\n",
        "        if iteration % 100 == 0:\n",
        "            losses_str = ['{:.4f}'.format(losses[i][-1]) for i in range(n_clients)]\n",
        "            accuracies_str = [to_percent(accuracies[i][-1]) for i in range(n_clients)]\n",
        "            print('Iteration={}, losses={}, accuracies={}'.format(iteration, losses_str, accuracies_str))\n",
        "    plot_federated_graphs(losses, accuracies)\n",
        "    test_acc = compute_accuracy(model, test_input, test_output)\n",
        "    print('\\nTesting Accuracy = {}'.format(to_percent(test_acc)))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7u179aem26R"
      },
      "source": [
        "model = federated_learning(client_features, client_targets, test_input, test_output1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}